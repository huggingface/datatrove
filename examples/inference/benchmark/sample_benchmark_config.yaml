# Sample benchmark configuration for vLLM throughput experiments
#
# This config demonstrates how to set up a sweep over different models,
# tensor parallelism (TP) and pipeline parallelism (PP) configurations.
#
# Launch with:
#   python examples/inference/benchmark/launch_experiments.py --config examples/inference/benchmark/sample_benchmark_config.yaml
#
# Dry run (no submission):
#   python examples/inference/benchmark/launch_experiments.py --config examples/inference/benchmark/sample_benchmark_config.yaml --dry-run

script: "examples/inference/generate_data.py"
continue_on_failure: true

# Fixed arguments applied to all runs
fixed_args:
  qos: "high"
  time: "1:00:00"

  model-max-context: 2048
  max-tokens: 1024

  input-dataset-name: "simplescaling/s1K-1.1"
  input-dataset-split: "train"
  prompt-column: "question"
  
  output-dataset-name: "s1K-1.1-benchmark"
  output-dir: "examples/inference/benchmark/results"

  benchmark-mode: true
  
  workers: 1
  tasks: 1
  max-num-seqs: 1000
  examples-per-chunk: 50
  trust-remote-code: true

  tp: 1
  pp: 1
  dp: 1

# Experiments - each experiment can spawn multiple runs (SLURM jobs)
# List values in args are expanded into the cartesian product of configurations.
# Run names are auto-derived from model, TP, PP, and other config parameters.
experiments:
  # Compact models
  - name: "1B-models"
    args:
      model-name-or-path: 
        - "google/gemma-3-1b-it"
        - "Qwen/Qwen3-1.7B"
  - name: "4B-models"
    args:
      model-name-or-path: 
        - "google/gemma-3-4b-it"
        - "Qwen/Qwen3-4B"
      tp: [1, 2]
  # Medium models
  - name: "Medium-models"
    args:
      model-name-or-path: 
        - "openai/gpt-oss-20b"
        - "Qwen/Qwen3-30B-A3B-Thinking-2507"
        - "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"
        - "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8"
        - "Qwen/Qwen3-Next-80B-A3B-Thinking"
      tp: [1, 2, 4]
      quantization: [None, bitsandbytes]
      kv-cache-dtype: [auto, fp8_e4m3, fp8_e5m2]
      speculative-config: 
        - None
        - '{"method": "ngram", "num_speculative_tokens": 6}'
        - '{"method": "suffix", "num_speculative_tokens": 16}'
  # Large models
  - name: "gpt-oss-120b"
    args:
      model-name-or-path: "openai/gpt-oss-120b"
      tp: [2, 4, 8]
      speculative-config: [None]
  - name: "Qwen3-235B-A22B-Thinking-2507"
    args:
      model-name-or-path: "Qwen/Qwen3-235B-A22B-Thinking-2507"
      tp: [8]
      speculative-config: [None]
  # Enormous models
  - name: "Kimi-K2-Instruct"
    args:
      model-name-or-path: "moonshotai/Kimi-K2-Instruct"
      tp: [8]
      pp: [2]
      nodes-per-task: [2]
      speculative-config: [None]
      examples-per-chunk: 10
      max-num-seqs: 16
